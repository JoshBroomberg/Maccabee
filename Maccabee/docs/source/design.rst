Introducing Maccabee
=====================

This package facilitates the use of a new approach to benchmarking causal inference methods: benchmarking performed by combining empirical covariates and sampled synthetic treatment and outcome functions.

Most existing benchmarks fall into one of two categories:

1. Empirical: using observed covariate and outcome data from experiments. In this case there is some true Data Generating Process (:term:`DGP`) but it is latent and unknown. The true treatment effect is recovered through randomization in the original experiment.

2. Synthetic: using covariate and outcome data generated by manually specified functions. Here, the DGP is known and the true effect can be recovered directly from the DGP.

The common flaw present in both benchmarking designs is that the sample size is, effectively, one. A new causal inference method is usually tested against a single DGP. In empirical datasets, the properties of this DGP and, most importantly, its place in the :term:`distributional problem space` is unknown. So the causal inference method is validated against a single sample from an unknown location in the problem space. In synthetic datasets, the DGP and the location is problem space is known. But, in the worst case, the manual specification introduces specification bias - where the best/simplest version of a DGP in a problem space region is selected (perhaps unintentially). Even in the best case, there is only one (or a few) samples from the location in problem space, potentially missing important aspects of the distribution of method performance over structurally similar DGP. Finally, synthetic DGPs rely on unrealistic (synthetic) covariate data which means that their results may not generalize to real datasets with similar treatment/outcome mechanisms.

The approach proposed in this package is designed to improve the validity of benchmarks by more thoroughly exploring the distributional problem space (relative to empirical benchmarks) and reducing specification bias (relative to synthetic benchmarks) while using realistic covariate distributions. Most importantly, sampling Data Generating Processes allows one to evaluate the distribution over method performance in a region of the :term:`distributional problem space`.

Design Principles
===================

Fundamentally, this package only succeeds if it provides a useful and usable way to benchmark new methods for causal inference developed by its users. Maccabee’s features are focused around four design principles to achieve this end:

* **Minimal imposition on method design:** attention has been paid to ensuring model developers can use their own empirical data and models with Maccabee painlessly. This includes support for benchmarking models written in both Python and R to avoid the need for language translation.

* **Quickstart but powerful customization:** The package includes high-quality data and pre-tuned parameters. This means that little boilerplate code is required to run a benchmark and receive results. This helps new users understand, and get value out of, the package quickly. At the same time, there is a large control surface to give advanced users the tools they need to support heavily-customized benchmarking processes.

* **Support for advanced functionality:** all Monte Carlo benchmarking requires access to sufficient computational power and a way to persist and compare results. Maccabee provides seamless integration with cluster computing tools to run large benchmarks on public cloud/private compute platforms as well as providing tools for result persistence and management which work both locally and with cluster computing.

* **Smooth side-by-side support of old and new approaches:** most users may feel initial discomfort using only the novel benchmarking approach proposed in the theoretical work. Maccabee allows for concrete, user-specified DGPs to be used side by side with the new approach. This allows users to switch between/compare the new and old approaches while using a single benchmarking tool. It also allows users to exploit the advanced functionality outlined above even if they don’t use the core sampling functionality. The hope is that users who start with concrete DGPs will transition to the newer (and theoretically superior) sampling approaches.

Theoretical Approach
======================

TODO: flesh this out.

Discuss the position of data/DGPs in distributional problem space, using parameterized sampling of functions to hit positions in space, the idea of transformed covariates as it relates to sample functions(NB), the idea of observable vs oracle information.

High-Level Object Design
=========================

TODO: flesh this out.

At a 10000 feet, the Maccabee package works as below. The concepts which are represented by Maccabee classes are bolded and presented with links to the relevant documentation.

To perform a **Benchmark** (:mod:`~maccabee.benchmarking`), one or more sets of **Sampling Parameters** (:mod:`~maccabee.parameters`) are using by the **DGP Sampler** (:mod:`~maccabee.data_generation.data_generating_process_sampler`) to sample **DGPs** (:mod:`~maccabee.data_generation.data_generating_process`) at a specific location in the :term:`distributional problem space`. **Data sets** (:mod:`~maccabee.data_generation.generated_data_set`) are then sampled from the sampled DGPs. The location of these data sets in the problem space is evaluated using **Data Metrics** (:mod:`~maccabee.data_analysis.data_metrics`). **Causal Models** (:mod:`~maccabee.modeling.models`) are used to generate estimates for a selected causal estimands. The performance of the models is evaluated against the ground truth from the sampled data sets using **Performance Metrics** (:mod:`~maccabee.modeling.performance_metrics`). The results of repeated DGP and data set samples are aggregated and returned to the user.

TODO: supplement/replace with a figure.

TODO: cover:

* Flexible parameter specification
* DGP Sampling
* DGP spec - DSL

Glossary
========

TODO: improve these.

.. glossary::

    causal model
      A causal model implements a mathematical estimator which extracts a causal estimand from an observational data set.

    data metric
      Data Metrics are real-valued functions which measure some distributional property of a generated data set. Each data metric measures the position of the data set along some well-defined 'axis' of the distributional problem space. Each axis may have more than one corresponding data metric.

    DGP
      A Data Generating Process describes the mathematical process which gives rise to a set of observed data - covariates, treatment assignments, and outcomes - and the corresponding unobserved/oracle data, primarily the treatment effect.

      Concretely, a DGP relates the DGP Variables - defined in the constants group :class:`~maccabee.constants.Constants.DGPVariables` - through a series of stochastic/deterministic functions. The nature of these functions defines the location of the resultant data sets in the :term:`distributional problem space`.

    distributional problem space
      The performance of causal estimators depends on distributional properties of the observed data. The space of all possible distributional properties forms the distributional problem space. The performance of an estimator across the space and in specific regions is of interest to researchers.

    distributional problem space axis
      The :term:`distributional problem space` is defined by axes which represent the distributional properties and the values they can take on. The cartesian product of the values the axes can take out is the extent of the problem space.

    dsl
      TODO - domain specific language.

    dgp variable
      DGP variables are the variables over which the DGP is defined. See chapter 3 and 4 of the theory work.

    observable dgp variable
      DGP variables which are available for causal inference.

    oracle dgp variable
      DGP variables which are not available for causal inference but which can be thought of as 'existing' during the data generation process. This includes potential outcomes, treatment effect, outcome noise etc.

    parameter specification file
      A file used to specify a set of DGP sampling parameters. The specification conforms to the schema laid out in the :term:`parameter schema file`.

    default parameter specification file
      The file which specifies the default set of DGP sampling parameters. This is laid out as a standard :term:`parameter specification file`.

    parameter schema file
      The file which defines all of the DGP sampling parameters by providing names, types, validity conditions, and descriptions. The :term:`parameter specification file` specifies DGP sampling parameters that conform to the schema laid out in this file.

    performance metric
      Performance Metrics are real-valued functions which measure the quality of a causal estimator by comparing the estimand value to the ground truth. A performance metric may be well defined for a single estimand value but typically, in the context of this package, they are defined over a sample of estimand values with each estimand value corresponding to an estimate of the causal effect/s in a generated data set.

    transformed covariate
      TODO - transformed covariate

    YML
      YAML is a human-readable data-serialization language. It is commonly used for configuration files and in applications where data is being stored or transmitted (Wikipedia).
