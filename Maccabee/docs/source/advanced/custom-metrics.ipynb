{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbsphinx": "hidden",
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Config Cell\n",
    "\n",
    "#!python -m pip install -e ../../../../Maccabee > /dev/null"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Custom Metrics\n",
    "**************\n",
    "\n",
    "It's possible to add both custom performance and custom data metrics to Maccabee. These metrics can then be collected alongside the built in metrics during benchmarking. The process for adding each kind of metric is outlined below.\n",
    "\n",
    "Custom Performance Metrics\n",
    "--------------------------\n",
    "\n",
    "The function :func:`~maccabee.modeling.performance_metrics.add_performance_metric` function can be used to add new performance metrics. This function takes a metric aggregation level, a metric name and a metric callable. See the function reference docs for more details on these arguments.\n",
    "\n",
    "The code below adds a new performance metric. The new metric is the mean absolute error for average effect estimates. The mathematical formula for this metric is given below, using the notation from the :mod:`~maccabee.modeling.performance_metrics` module.\n",
    "\n",
    ".. math::\n",
    "    \\frac{1}{N} \\sum_i \\left| \\hat{\\tau_i} - \\tau_i \\right|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maccabee.modeling.performance_metrics import add_performance_metric\n",
    "from maccabee.constants import Constants\n",
    "\n",
    "\n",
    "perf_metric_name = \"MAE\"\n",
    "\n",
    "def mean_absolute_error(avg_effect_estimate_vals, avg_effect_true_vals):\n",
    "    import numpy as np\n",
    "    return np.mean(np.abs(avg_effect_estimate_vals - avg_effect_true_vals))\n",
    "    \n",
    "add_performance_metric(\n",
    "    aggregation_level=Constants.Model.AVERAGE_ESTIMANDS,\n",
    "    metric_name=perf_metric_name,\n",
    "    metric_callable=mean_absolute_error)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "\n",
    "Custom Data Metrics\n",
    "--------------------\n",
    "\n",
    "The function :func:`~maccabee.data_analysis.data_metrics.add_data_metric` function can be used to add new data metrics. This function takes an axis name, a metric specification dictionary. See the function reference docs for more details on these arguments.\n",
    "\n",
    "The code below adds a new data metric. The new metric is based on a function which combines the untreated potential outcome, the outcome noise and a constant offset and then regresses the result onto the observed covariates. This demonstrates the ability to access DGP variables and static values and combine them using arbitrary Python code to create new metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define axes and metrics to analyze\n",
    "from maccabee.data_analysis.data_metrics import add_data_metric\n",
    "from maccabee.constants import Constants\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Metric for the outcome nonlinearity axis\n",
    "data_axis = Constants.AxisNames.OUTCOME_NONLINEARITY\n",
    "\n",
    "# Define the callable function\n",
    "def noisy_biased_outcome_linearity_metric(X, Y0, outcome_noise, bias):\n",
    "    target = Y0 + outcome_noise + bias\n",
    "    lr = LinearRegression().fit(X, target)\n",
    "    return lr.score(X, target)\n",
    "    \n",
    "# Define the metric dict\n",
    "data_metric_name = \"lin (X, Noisy, Biased Y)\"\n",
    "metric_dict = {\n",
    "    \"name\": data_metric_name,\n",
    "    \"args\": {\n",
    "        \"X\": Constants.DGPVariables.COVARIATES_NAME,\n",
    "        \"Y0\": Constants.DGPVariables.POTENTIAL_OUTCOME_WITHOUT_TREATMENT_NAME,\n",
    "        \"outcome_noise\": Constants.DGPVariables.OUTCOME_NOISE_NAME   \n",
    "    },\n",
    "    \"constant_args\": {\n",
    "        \"bias\": 42\n",
    "    },\n",
    "    \"function\": noisy_biased_outcome_linearity_metric\n",
    "}\n",
    "\n",
    "add_data_metric(data_axis, metric_dict)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Accessing Custom Metrics\n",
    "------------------------\n",
    "\n",
    "We can now run a benchmark and show that our new performance and data metrics are immediately usable. The custom performance metric is automatically evaluated for all appropriate estimands. The custom data metric is selectable when running the benchmarking in `data_analysis_mode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from maccabee.data_sources.data_source_builders import build_random_normal_datasource\n",
    "from maccabee.benchmarking import benchmark_model_using_sampled_dgp\n",
    "from maccabee.modeling.models import LinearRegressionCausalModel\n",
    "from maccabee.parameters import build_default_parameters\n",
    "\n",
    "# Build the parameters\n",
    "params = build_default_parameters()\n",
    "\n",
    "# Build a random normal data source\n",
    "normal_data_source = build_random_normal_datasource(\n",
    "    n_covars=5,\n",
    "    n_observations=1000)\n",
    "\n",
    "# Select the new data metric (and an old one for good measure)\n",
    "DATA_METRICS_SPEC = {\n",
    "    Constants.AxisNames.OUTCOME_NONLINEARITY: [\n",
    "        data_metric_name,\n",
    "        \"Lin r2(X_obs, Y0)\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Run a benchmark\n",
    "perf_agg_metrics, _, _, data_agg_metrics, _, _ = benchmark_model_using_sampled_dgp(\n",
    "    dgp_sampling_params=params,\n",
    "    model_class=LinearRegressionCausalModel,\n",
    "    estimand=Constants.Model.ATE_ESTIMAND,\n",
    "    data_source=normal_data_source,\n",
    "    num_dgp_samples=10,\n",
    "    num_sampling_runs_per_dgp=1,\n",
    "    num_samples_from_dgp=16,\n",
    "    data_analysis_mode=True, # SET DATA ANALYSIS MODE\n",
    "    data_metrics_spec=DATA_METRICS_SPEC, # PROVIDE SPEC\n",
    "    n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New performance metric\n",
    "perf_agg_metrics[perf_metric_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.476"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New data metric\n",
    "data_agg_metrics[f\"{data_axis} {data_metric_name}\"]"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
